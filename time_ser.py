# -*- coding: utf-8 -*-
"""3lab_Eidelman_468152.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vOJstTDdqJLE_PXwLGMZKULkrQtuGQz4

Эйдельман К.В.
468152

Появилось время на GPU, доучил

# Импорт библиотек и загрузка датасета
"""

import pandas as pd
import requests
import sklearn
import torch
import matplotlib.pyplot as plt
import numpy as np
import random
from sklearn.preprocessing import StandardScaler
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import torch.nn.init as init
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.metrics import mean_absolute_error, r2_score

df_etalon_for_test = pd.read_csv("https://storage.yandexcloud.net/yanchick/etalon_for_test_t.csv", header=None).values.T
df_etalon_for_train = pd.read_csv("https://storage.yandexcloud.net/yanchick/etalon_for_train_t.csv", header=None).values.T
df_test = pd.read_csv("https://storage.yandexcloud.net/yanchick/test_t.csv", header=None).values.T
df_train = pd.read_csv("https://storage.yandexcloud.net/yanchick/train_t.csv", header=None).values.T

"""Для воспроизводимости результатов зафиксируем параметры генераторов случайных чисел.

"""

SEED = 42

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True

"""Укажем утройство"""

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
device

"""# Задача 1. Подготовка датасета"""

print(df_train.shape)
print(df_etalon_for_train.shape)

scaler = StandardScaler()

# Функция для нормализации каждой реализации
def normalize_series(series):
    return scaler.fit_transform(series.reshape(-1, 1)).flatten()

# Применяем ко всем данным
train_data_normalized = np.array([normalize_series(series) for series in df_train])
test_data_normalized = np.array([normalize_series(series) for series in df_test])
etalon_train_normalized = np.array([normalize_series(series) for series in df_etalon_for_train])
etalon_test_normalized = np.array([normalize_series(series) for series in df_etalon_for_test])

"""Чтобы убедиться, что все корректно, выведем несколько графиков"""

# Построим графики нескольких реализаций из обучающей выборки
plt.figure(figsize=(12, 6))
for i in range(6):
    plt.plot(train_data_normalized[i], label=f'Реализация {i+1}')
plt.title("Примеры реализаций из обучающей выборки")
plt.ylabel("Значение измерения")
plt.legend()
plt.show()

def plot_example(X, y, r):
    plt.figure(figsize=(18, 12))

    for i, idx in enumerate(r, 1):
        plt.subplot(2, 3, i)  # 2 строки, 3 столбца, текущий подграфик i
        plt.plot(X[idx], label='Зашумленные данные', alpha=0.7)
        plt.plot(y[idx], label='Истинный сигнал', linewidth=1.5)
        plt.title(f'Сигнал {idx}')
        plt.ylabel("Значение")
        plt.legend()

    plt.tight_layout()
    plt.suptitle("Сравнение зашумленных данных и истинного сигнала для 6 реализаций", y=1.02)
    plt.show()

random_indices = random.sample(range(1000), 6)
plot_example(train_data_normalized, etalon_train_normalized, random_indices)

class TimeSeriesDataset(Dataset):
    def __init__(self, data, seq_length):

        self.data = data  # [1000, 5000]
        self.seq_length = seq_length

    def __len__(self):
        return len(self.data)  # 1000 реализаций

    def __getitem__(self, idx):
        series = self.data[idx]  # Вся реализация (5000 точек)
        # Возвращаем всю последовательность и все целевые точки
        return (
            torch.FloatTensor(series[:-1]),  # Все точки кроме последней
            torch.FloatTensor(series[1:])    # Все точки кроме первой
        )

# Параметры
SEQ_LENGTH = 100
BATCH_SIZE = 32

train_dataset = TimeSeriesDataset(train_data_normalized, SEQ_LENGTH)
test_dataset = TimeSeriesDataset(test_data_normalized, SEQ_LENGTH)

# DataLoader с коллайтом для обработки последовательностей
def collate_fn(batch):
    xs, ys = zip(*batch)
    return torch.stack(xs), torch.stack(ys)

train_loader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True,
    collate_fn=collate_fn
)

"""# Создание модели

В данном случае, поскольку модель сигнала неизвестна, попробуем использовать LSTM, так как она хорошо подходит для задач прогнозирования временных рядов
"""

class LSTMModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=1,
            hidden_size=64,
            num_layers=2,
            batch_first=True
        )
        self.linear = nn.Linear(64, 1)

    def forward(self, x):
        # x: [batch, seq_len, 1]
        out, _ = self.lstm(x)  # [batch, seq_len, 64]
        return self.linear(out)  # [batch, seq_len, 1]

def train_model(model, loss_fun, optimizer, train_loader, epochs=20, clip_grad=None, patience=3, factor=0.1):
    device = next(model.parameters()).device
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience, verbose=True)

    print(f"\nНачало обучения на: {device}")
    print(f"Всего реализаций: {len(train_loader.dataset)}")
    print(f"Размер батча: {train_loader.batch_size}")
    print(f"Итераций на эпоху: {len(train_loader)}")
    print(f"Начальный LR: {optimizer.param_groups[0]['lr']:.2e}")

    losses = []
    best_loss = float('inf')
    no_improve = 0

    # Включение mixed precision если доступно
    use_amp = torch.cuda.is_available()
    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)

    for epoch in range(epochs):
        model.train()
        epoch_loss = 0

        with tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}", unit="batch") as pbar:
            for sequences, targets in pbar:
                sequences = sequences.to(device, non_blocking=True)
                targets = targets.to(device, non_blocking=True)

                optimizer.zero_grad(set_to_none=True)

                # Forward pass с mixed precision
                with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):
                    outputs = model(sequences.unsqueeze(-1))  # [batch, seq_len, 1]
                    loss = loss_fun(outputs.squeeze(-1), targets)

                # Backward pass
                scaler.scale(loss).backward()

                if clip_grad:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)

                scaler.step(optimizer)
                scaler.update()

                # Обновление статистики
                epoch_loss += loss.item()
                pbar.set_postfix({"batch_loss": f"{loss.item():.4f}"})

                # Очистка памяти каждые 50 батчей
                if pbar.n % 50 == 0 and torch.cuda.is_available():
                    torch.cuda.empty_cache()

        # Средний лосс за эпоху
        avg_loss = epoch_loss / len(train_loader)
        losses.append(avg_loss)

        # Обновление шедулера
        scheduler.step(avg_loss)

        # Проверка улучшения
        if avg_loss < best_loss:
            best_loss = avg_loss
            no_improve = 0
        else:
            no_improve += 1

        print(f"Epoch {epoch+1} | Loss: {avg_loss:.4f} | "
              f"LR: {optimizer.param_groups[0]['lr']:.2e}")

    return losses

model = LSTMModel().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

losses = train_model(
    model=model,
    loss_fun=nn.MSELoss(),
    optimizer=optimizer,
    train_loader=train_loader,
    epochs=50,
    clip_grad=1.0,
    patience=5,
    factor=0.5
)

def print_loss(losses):
    plt.plot(losses)
    plt.xlabel("Epoches")
    plt.ylabel("Loss")

print_loss(losses)

"""# Оценка результатов"""

def evaluate_model(model, test_loader, loss_fn):
    model.eval()
    total_loss = 0
    predictions = []
    actuals = []

    with torch.no_grad():
        for sequences, targets in test_loader:
            sequences = sequences.to(device).unsqueeze(-1)
            targets = targets.to(device)

            outputs = model(sequences)
            loss = loss_fn(outputs.squeeze(-1), targets)
            total_loss += loss.item()

            # Сохраняем для визуализации
            predictions.append(outputs.cpu().numpy())
            actuals.append(targets.cpu().numpy())

    avg_loss = total_loss / len(test_loader)
    print(f'Test Loss: {avg_loss:.4f}')

    # Конкатенация всех батчей
    predictions = np.concatenate(predictions)
    actuals = np.concatenate(actuals)

    return avg_loss, predictions, actuals

# Создаем тестовый DataLoader
test_dataset = TimeSeriesDataset(test_data_normalized, seq_length=100)
test_loader = DataLoader(
    test_dataset,
    batch_size=32,
    shuffle=False,
    collate_fn=collate_fn
)

# Проверка модели
test_loss, preds, actuals = evaluate_model(model, test_loader, nn.MSELoss())

def plot_predictions(predictions, actuals, etalon_test, num_samples=1):
    plt.figure(figsize=(15, 5))
    for i in range(num_samples):
        idx = np.random.randint(0, len(predictions))
        plt.plot(actuals[idx], label='Actual')
        plt.plot(predictions[idx], label='Predicted', alpha=0.7)
        plt.plot(etalon_test[idx], label='True', alpha=0.8)
        plt.title(f'Sample {i+1}')
        plt.legend()
        plt.show()

plot_predictions(preds, actuals, etalon_test_normalized)

def calculate_metrics(predictions, actuals):
    # Для всех точек сразу
    flat_preds = predictions.flatten()
    flat_actuals = actuals.flatten()

    mae = mean_absolute_error(flat_actuals, flat_preds)
    r2 = r2_score(flat_actuals, flat_preds)

    print(f'MAE: {mae:.4f}')
    print(f'R2 Score: {r2:.4f}')

calculate_metrics(preds, actuals)

"""Видна периодичность, но с запаздыванием. Очень плохой результат.

# Фильтрация
Для лучшего эффекта, попробуем отфильтровать сигналы тренирвочные и тестовые
"""

from scipy.fft import fft, fftfreq

np.random.seed(42)
sample_indices = np.random.choice(len(df_train), 6, replace=False)
plt.figure(figsize=(15, 15))

# Частота дискретизации (0.05 с между отсчетами)
fs = 1 / 0.05  # 20 Гц

for i, idx in enumerate(sample_indices, 1):
    plt.subplot(3, 2, i)

    # Чистый сигнал
    clean_signal = df_etalon_for_train[idx]
    n = len(clean_signal)
    yf_clean = fft(clean_signal)
    xf = fftfreq(n, 1/fs)[:n//2]
    plt.plot(xf, 2/n * np.abs(yf_clean[0:n//2]),
             'b-', label='Чистый сигнал', alpha=0.7)

    # Зашумленный сигнал
    noisy_signal = df_train[idx]
    yf_noisy = fft(noisy_signal)
    plt.plot(xf, 2/n * np.abs(yf_noisy[0:n//2]),
             'r-', label='Зашумленный', alpha=0.5)

    plt.title(f'Реализация {idx}')
    plt.xlabel('Частота (Гц)')
    plt.ylabel('Амплитуда')
    plt.legend()
    plt.xlim((0,1.5))

plt.suptitle('Сравнение спектров чистого и зашумленного сигналов')
plt.show()

"""Исходя из графиков видно, что полезная часть сигнала находится в диапазоне до 0.4-0.5 Гц. Соответственно, следует построить ФНЧ с полосой пропускания до 0.4 Гц"""

cutoff = 0.4  # Частота среза [Гц]
order = 5     # Порядок фильтра

from scipy import signal
# Создаем НЧ фильтр Баттерворта
b, a = signal.butter(order, cutoff/(fs/2), btype='low')

# Функция для фильтрации сигнала
def lowpass_filter(data):
    return signal.filtfilt(b, a, data)

np.random.seed(42)
sample_indices = np.random.choice(len(df_train), 1, replace=False)
for i, idx in enumerate(sample_indices, 1):
    # Получаем сигналы
    noisy_signal = df_train[idx]
    clean_signal = df_etalon_for_train[idx]

    # Фильтруем зашумленный сигнал
    filtered_signal = lowpass_filter(noisy_signal)

    # Вычисляем спектры
    def compute_spectrum(x):
        n = len(x)
        yf = fft(x)
        xf = fftfreq(n, 1/fs)[:n//2]
        return xf, 2/n * np.abs(yf[0:n//2])
    # Чистый сигнал
    xf, clean_fft = compute_spectrum(clean_signal)
    plt.plot(xf, clean_fft, 'b-', label='Чистый сигнал', alpha=0.8)

    # Зашумленный сигнал
    _, noisy_fft = compute_spectrum(noisy_signal)
    plt.plot(xf, noisy_fft, 'r-', label='Зашумленный', alpha=0.4)

    # Фильтрованный сигнал
    _, filtered_fft = compute_spectrum(filtered_signal)
    plt.plot(xf, filtered_fft, 'g--', label='Фильтрованный', alpha=0.8)

    # Настройки графика
    plt.xlabel('Частота [Гц]')
    plt.ylabel('Амплитуда')
    plt.xlim(0, 2)  # Увеличиваем масштаб для наглядности
    plt.legend(loc='upper left')

"""Фильтрация работает. Необходимо профильтровать остальные сигналы"""

# Фильтрация всех обучающих данных
filtered_train = np.apply_along_axis(lowpass_filter, 1, train_data_normalized)

# Фильтрация всех тестовых данных
filtered_test = np.apply_along_axis(lowpass_filter, 1, test_data_normalized)

random_indices = random.sample(range(1000), 6)
plot_example(filtered_train, df_etalon_for_train, random_indices)

"""Видно колосальное различие между исходными данными и фильтрованными

# Обучение второй модели
"""

train_dataset2 = TimeSeriesDataset(filtered_train, SEQ_LENGTH)
test_dataset2 = TimeSeriesDataset(filtered_test, SEQ_LENGTH)

test_loader2 = DataLoader(
    test_dataset2,
    batch_size=32,
    shuffle=True,
    collate_fn=collate_fn
)

train_loader2 = DataLoader(
    train_dataset2,
    batch_size=32,
    shuffle=True,
    collate_fn=collate_fn
)

model2 = LSTMModel().to(device)
optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)

losses2 = train_model(
    model=model2,
    loss_fun=nn.MSELoss(),
    optimizer=optimizer2,
    train_loader=train_loader2,
    epochs=50,
    clip_grad=1.0,
    patience=5,
    factor=0.5
)

print_loss(losses2)

test_loss, preds2, actuals2 = evaluate_model(model2, test_loader2, nn.MSELoss())

plt.plot(preds2[0])
plt.plot(actuals2[0])

calculate_metrics(preds2, actuals2)

"""Видно, что фильтрация очень помогла для распознавания"""